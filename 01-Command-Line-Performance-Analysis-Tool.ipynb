{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549339c3",
   "metadata": {},
   "source": [
    "# Performance Analysis Profiling on CPU and GPU cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1cc8f",
   "metadata": {},
   "source": [
    "Welcome to the webinar _Performance Analysis Profiling on CPU and GPU cores_ in . In this webinar you will learn several techniques for profiling single CPU and GPU applications with an emphasis on supercomputing environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2465e",
   "metadata": {},
   "source": [
    "## The Coding Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b909c",
   "metadata": {},
   "source": [
    "The first step is display information about the CPU architecture with the command `lscpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8685bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea3b9c",
   "metadata": {},
   "source": [
    "In this node, we can observe that the multi-GPU resources connect with the NUMA nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eecce9",
   "metadata": {},
   "source": [
    "For your work today, you have access to several GPUs in the cloud. Run the following cell to see the GPUs available to you today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi topo -m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65989834",
   "metadata": {},
   "source": [
    "While your work today will be on a single node, all the techniques you learn today, in particular CUDAWARE-MPI and NVSHMEM, can be used to run your applications across clusters of multi-GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfba876",
   "metadata": {},
   "source": [
    "Let us show the NVLink Status for different GPUs reported from `nvidia-smi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338366d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi nvlink --status -i 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666c4e4",
   "metadata": {},
   "source": [
    "In the end, it gives information about the NUMA memory nodes, with tue `lstopo` command, that is used to show the topology of the system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f054097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lstopo --of png > ogbon.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57470e88-3846-4cb2-9bb0-3edad6cbc2b1",
   "metadata": {},
   "source": [
    "This will import and display a .png image in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb812d0f-7679-4bec-8f0f-aca17b3d9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "path=\"ogbon.png\"\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ce682-4e00-4497-aecf-a9c4d7bb5e16",
   "metadata": {},
   "source": [
    "## Environment Modules on OGBON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6b8ef-a6b8-40b5-9eb1-b939d63bd381",
   "metadata": {},
   "source": [
    "```cpp\n",
    "Currently Loaded Modulefiles:\n",
    "    1) anaconda3/2022.05 \n",
    "    2) cuda/11.6         \n",
    "    3) ucx/1.12.0-cuda-11.6-ofed-5.4\n",
    "    4) gcc/11.1.0  \n",
    "    5) openmpi/4.1.1-cuda-11.6-ofed-5.4\n",
    "    6) intel/parallel-studio-xe/2020.2        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdef8d4-7b1f-4c55-950e-58b64a2e9f64",
   "metadata": {},
   "source": [
    "## Profiling CPU cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbcfa5-07ca-4266-8580-3acd1df804e2",
   "metadata": {},
   "source": [
    "Profiling sequential algorithms in supercomputational environments, it is necessary to measure the code points that require the highest computational cost of the application so that we can focus our efforts on parallelizing these sections; in this way, we can work intelligently where the code needs to gain performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0619a06b-4d15-4d22-9886-d5a225e830a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_cpucores.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "void func1(void)\n",
    "{    \n",
    "    int n = 1000;  \n",
    "    int i,j,k;\n",
    "\n",
    "    int  *A = (int *) malloc (sizeof(int)*n*n);\n",
    "    int  *B = (int *) malloc (sizeof(int)*n*n);\n",
    "    int  *C = (int *) malloc (sizeof(int)*n*n);\n",
    "\n",
    "    for(i=0; i < n; i++){\n",
    "      for(j=0; j < n; j++){\n",
    "        A[i*n+j] = rand()%(10-1)*1;\n",
    "        B[i*n+j] = rand()%(10-1)*1;\n",
    "      }\n",
    "    }\n",
    "\n",
    "    for(i = 0; i < n; i++) \n",
    "     for(j = 0; j < n; j++)\n",
    "       for( k = 0; k < n; k++) \n",
    "        C[i*n+j]+=A[i*n+k]*B[k*n+j]; \n",
    "\n",
    "    return;\n",
    "}\n",
    "\n",
    "void func2(void)\n",
    "{\n",
    "    double h, x, s = 0, result;\n",
    "    int a = 0, b = 1;\n",
    "    int n = 1000000;\n",
    "    int i;\n",
    "\n",
    "    h = (b - a) / n;\n",
    "\n",
    "    for(i = 0; i < n; i++) \n",
    "    {\n",
    "       x = (a + h * (i + 0.5));\n",
    "       s += 100 * x + sin(2 * x * 3.14159);\n",
    "    }\n",
    "\n",
    "    result = h * s;\n",
    "    \n",
    "    return;\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "    printf(\"Inside main()\\n\");\n",
    "\n",
    "    printf(\"Inside func1()\\n\");\n",
    "    func1();\n",
    "    \n",
    "    printf(\"Inside func2()\\n\");\n",
    "    for(int i = 0; i < 500; i++)\n",
    "      func2();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51956e-198a-4b9e-893a-b364813ecf33",
   "metadata": {},
   "source": [
    "### GPROF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b437f-a104-492e-bfc4-01382e08816b",
   "metadata": {},
   "source": [
    "GNU profiler (gprof) will be used,  whose primary function is to analyze and capture times during code execution, generating performance reports on multiprocessor environments. To execute the profiling process, insert the _-pg_ argument in the compilation of our sequential code, run it usually to generate the binary file of the report, and, soon after, display it in a readable way through the command associated with gprof, illustrates up as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7593d2-157e-496b-ae76-eefb81c549b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc test_cpucores.c -o test_gprof -lm -pg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b495b3-ab3c-4894-a07f-4d28a06d37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./test_gprof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff87a9-476f-445c-9fff-9b6c25049921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gprof -b test_gprof gmon.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232479fe-5e87-427a-9a8d-682bbe81c000",
   "metadata": {},
   "source": [
    "### VTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8093c4-69a8-4ee4-8000-511aafd4269d",
   "metadata": {},
   "source": [
    "The Intel(R) VTune(TM) Profiler Command Line Tool (vtune) perform the hotspots collection based on user mode sampling on the given target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83299e4-dbbe-40b7-bdcb-0220a6729bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc test_cpucores.c -o test_vtune -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a4f29-58ae-4103-a1f5-ee9f2a050f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vtune -collect hotspots ./test_vtune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e334a-32ce-440e-b4f6-80731e9f2d2e",
   "metadata": {},
   "source": [
    "### PERF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a36e77-3cbd-4d7d-b2d7-258656b8981d",
   "metadata": {},
   "source": [
    "The Perf Tool Performs (perf) performance analysis using counters, mainly referring to cache memories. A simple matrix multiply in the following can show this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0435e8-5235-4b33-9122-d1f062c1788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mm.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "void initializeMatrix(int *A, int n){\n",
    "\n",
    "  for(int i=0; i < n; i++)\n",
    "    for(int j=0; j < n; j++)\n",
    "      A[i*n+j] = rand()%(10-1)*1;\n",
    "  \n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    " int n = atoi(argv[1]);  \n",
    " int i,j,k;\n",
    "\n",
    " int  *A = (int *) malloc (sizeof(int)*n*n);\n",
    " int  *B = (int *) malloc (sizeof(int)*n*n);\n",
    " int  *C = (int *) malloc (sizeof(int)*n*n);\n",
    "\n",
    " initializeMatrix(A,n);\n",
    " initializeMatrix(B,n);\n",
    "\n",
    " for(j = 0; j < n;  j++)\n",
    "    for(i = 0; i < n; i++) \n",
    "      for(k = 0; k < n;  k++) \n",
    "          C[ i * n + j ] += A[ i * n + k ] * B[ k * n + j ];\n",
    "\n",
    "/*\n",
    " * TODO: Mensure the performance with the loop (i, j, k)\n",
    " */\n",
    "    \n",
    "/*\n",
    " * TODO: Mensure the performance with the loop (i, k, j)\n",
    " */\n",
    "\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e6c8b-55f1-4d33-a524-4ce1ee99eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc mm.c -o mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db34944-f7dd-44bf-8a13-eb9c4c5c984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf stat -d ./mm 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae792d-8fb8-40c7-80c3-9b93a2bb8445",
   "metadata": {},
   "source": [
    "After profiling the application with the loop i, j, k, answer the following questions using two new experiments:\n",
    "\n",
    "- Loop i, j, k;\n",
    "- Loop i, k, j;\n",
    "\n",
    "and answer the following questions using information displayed in the profiling before:\n",
    "\n",
    "- Were there any differences in code structure in performance? And if so, what would be the justification for this?\n",
    "- How does optimization relate to the concept of memory locality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46736b8-afe1-47c5-8a54-7263d943350a",
   "metadata": {},
   "source": [
    "## Profiling GPU cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c083f2-7f08-4fff-85b3-62d567ad9726",
   "metadata": {},
   "source": [
    "The GPU has many units working in parallel, and it is common for it to be bound by different units at different frame sequences. Due to the nature of this behavior, it is beneficial to identify where the GPU cost is going when searching for bottlenecks and to understand what a GPU bottleneck is. Some applications help developers identify bottlenecks, which is useful when optimizing performance, following some NVIDIA profiling tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb8908-8925-4ec2-b1ec-dda31759384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vector-add.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "void initWith(float num, float *a, int N)\n",
    "{\n",
    "  for(int i = 0; i < N; ++i)\n",
    "  {\n",
    "    a[i] = num;\n",
    "  }\n",
    "}\n",
    "\n",
    "__global__ \n",
    "void addVectorsInto(float *result, float *a, float *b, int N)\n",
    "{\n",
    "  int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  int stride = blockDim.x * gridDim.x;\n",
    "\n",
    "  for(int i = index; i < N; i += stride)\n",
    "  {\n",
    "    result[i] = a[i] + b[i];\n",
    "  }\n",
    "}\n",
    "\n",
    "void checkElementsAre(float target, float *vector, int N)\n",
    "{\n",
    "  for(int i = 0; i < N; i++)\n",
    "  {\n",
    "    if(vector[i] != target)\n",
    "    {\n",
    "      printf(\"FAIL: vector[%d] - %0.0f does not equal %0.0f\\n\", i, vector[i], target);\n",
    "      exit(1);\n",
    "    }\n",
    "  }\n",
    "  printf(\"Success! All values calculated correctly.\\n\");\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  const int N = 2<<24;\n",
    "  size_t size = N * sizeof(float);\n",
    "\n",
    "  float *a;\n",
    "  float *b;\n",
    "  float *c;\n",
    "\n",
    "  cudaMallocManaged(&a, size);\n",
    "  cudaMallocManaged(&b, size);\n",
    "  cudaMallocManaged(&c, size);\n",
    "\n",
    "  initWith(3, a, N);\n",
    "  initWith(4, b, N);\n",
    "  initWith(0, c, N);\n",
    "\n",
    "  size_t threadsPerBlock;\n",
    "  size_t numberOfBlocks;\n",
    "\n",
    "  int deviceId;\n",
    "  cudaGetDevice(&deviceId);\n",
    "\n",
    "  cudaDeviceProp props;\n",
    "  cudaGetDeviceProperties(&props, deviceId);\n",
    "  int multiProcessorCount = props.multiProcessorCount;\n",
    "  threadsPerBlock = 1024;\n",
    "  numberOfBlocks = 32 * multiProcessorCount;\n",
    "  \n",
    "  cudaError_t addVectorsErr;\n",
    "  cudaError_t asyncErr;\n",
    "\n",
    "  addVectorsInto<<<numberOfBlocks, threadsPerBlock>>>(c, a, b, N);\n",
    "\n",
    "  addVectorsErr = cudaGetLastError();\n",
    "  if(addVectorsErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(addVectorsErr));\n",
    "\n",
    "  asyncErr = cudaDeviceSynchronize();\n",
    "  if(asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr));\n",
    "\n",
    "  checkElementsAre(7, c, N);\n",
    "\n",
    "  cudaFree(a);\n",
    "  cudaFree(b);\n",
    "  cudaFree(c);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca8cda-960b-4784-aa8e-912bfe39dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc vector-add.cu -o vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043007a1-26d5-411c-ae0d-558765f1b77a",
   "metadata": {},
   "source": [
    "### NSYS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8d4ad-4689-4c1f-96b2-e80506bb4414",
   "metadata": {},
   "source": [
    "NVIDIA Nsight Systems (nsys) is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef04f2bc-fa7e-4322-8409-0e4a91849a36",
   "metadata": {},
   "source": [
    "The command `nsys profile` will generate a `qdrep` report file which can be used in a variety of manners. We use the `--stats=true` flag here to indicate we would like summary statistics printed. There is quite a lot of information printed:\n",
    "\n",
    "- Profile configuration details\n",
    "- Report file(s) generation details\n",
    "- **CUDA API Statistics**\n",
    "- **CUDA Kernel Statistics**\n",
    "- **CUDA Memory Operation Statistics (time and size)**\n",
    "- OS Runtime API Statistics\n",
    "\n",
    "In this lab you will primarily be using the nsys im command line. In the next, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac000e9-8814-42db-8872-9c9ed9df52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true ./vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39edc4cf-8e83-4e99-865f-796469ff3449",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `CUDA Kernel Statistics` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc374068-9b46-4b39-ada0-bdd04fa09f4d",
   "metadata": {},
   "source": [
    "### NCU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7858f4-59bf-4e98-86a7-2791ad193b88",
   "metadata": {},
   "source": [
    "NVIDIA Nsight Compute CLI (ncu) provides a non-interactive way to profile applications from the command line. It can print the results directly on the command line or store them in a report file. \n",
    "\n",
    "To print profiling information on the command line on the NCU, do not specify the output file (flag -o). Or, if you want to generate the output file (-o) and still see it on the command line, you can use the --page flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0091a-d8e0-41ad-a6dd-ef1b9bc98341",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu --set detailed vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e69674-ec46-4574-b4a8-bc626497ef4c",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "NextAfter profiling the application, with ncu, answer the following question:\n",
    "\n",
    "- What was the principal diference between nsys and ncu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e8e5be-0bb8-4693-8a9d-96c98b293220",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a305772-febf-4ebf-99cb-e6ad309f5a21",
   "metadata": {},
   "source": [
    "Please continue to the next notebook: [_Visual-Performance-Analysis-Tool_](02-Visual-Performance-Analysis-Tool.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
